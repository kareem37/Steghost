{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b61695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/20], E Loss: 0.1771, D Loss: 0.1771, Val Loss: 0.0700\n",
      "Epoch [2/20], E Loss: 0.0490, D Loss: 0.0490, Val Loss: 0.0380\n",
      "Epoch [3/20], E Loss: 0.0338, D Loss: 0.0338, Val Loss: 0.0287\n",
      "Epoch [4/20], E Loss: 0.0266, D Loss: 0.0266, Val Loss: 0.0253\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 180\u001b[0m\n\u001b[0;32m    177\u001b[0m     test_autoencoder(encoder, decoder, test_loader)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 180\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 169\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m AutoencoderLoss()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Train the Autoencoder\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m train_autoencoder(encoder, decoder, loss_fn, train_loader, val_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Load the best model for testing\u001b[39;00m\n\u001b[0;32m    172\u001b[0m best_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_autoencoder_params.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 85\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[1;34m(encoder, decoder, loss_fn, train_loader, val_loader, num_epochs, lr, data_depth)\u001b[0m\n\u001b[0;32m     82\u001b[0m optimizer_d\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Encode and decode images\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m encoded_images \u001b[38;5;241m=\u001b[39m encoder(images, random_data)\n\u001b[0;32m     86\u001b[0m decoded_data \u001b[38;5;241m=\u001b[39m decoder(encoded_images)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Autoencoder loss\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, image, data)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, data):\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([image, data], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "# Encoder model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, data_depth=3, hidden_size=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.data_depth = data_depth\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3 + data_depth, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_size, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, image, data):\n",
    "        x = torch.cat([image, data], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Decoder model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, data_depth=3, hidden_size=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.data_depth = data_depth\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_size, data_depth, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, encoded_image):\n",
    "        return self.model(encoded_image)\n",
    "\n",
    "# Custom loss function for the autoencoder\n",
    "class AutoencoderLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoencoderLoss, self).__init__()\n",
    "        self.pixelwise_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, decoded_data, real_data):\n",
    "        return self.pixelwise_loss(decoded_data, real_data)\n",
    "\n",
    "# Training function\n",
    "def train_autoencoder(encoder, decoder, loss_fn, train_loader, val_loader, num_epochs=20, lr=0.0002, data_depth=3):\n",
    "    encoder = encoder\n",
    "    decoder = decoder\n",
    "    loss_fn = loss_fn\n",
    "\n",
    "    optimizer_e = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    optimizer_d = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        e_loss_epoch = 0.0\n",
    "        d_loss_epoch = 0.0\n",
    "\n",
    "        for i, (images, _) in enumerate(train_loader):\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            random_data = torch.rand(batch_size, data_depth, images.size(2), images.size(3), device=images.device)\n",
    "\n",
    "            # Train Encoder and Decoder\n",
    "            optimizer_e.zero_grad()\n",
    "            optimizer_d.zero_grad()\n",
    "\n",
    "            # Encode and decode images\n",
    "            encoded_images = encoder(images, random_data)\n",
    "            decoded_data = decoder(encoded_images)\n",
    "\n",
    "            # Autoencoder loss\n",
    "            loss = loss_fn(decoded_data, random_data)\n",
    "            loss.backward()\n",
    "            optimizer_e.step()\n",
    "            optimizer_d.step()\n",
    "\n",
    "            e_loss_epoch += loss.item()\n",
    "            d_loss_epoch += loss.item()\n",
    "\n",
    "        # Validate\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, _ in val_loader:\n",
    "                batch_size = images.size(0)\n",
    "                random_data = torch.rand(batch_size, data_depth, images.size(2), images.size(3), device=images.device)\n",
    "                encoded_images = encoder(images, random_data)\n",
    "                decoded_data = decoder(encoded_images)\n",
    "                loss = loss_fn(decoded_data, random_data)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = {'lr': lr, 'epoch': epoch, 'encoder_state_dict': encoder.state_dict(), 'decoder_state_dict': decoder.state_dict()}\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], E Loss: {e_loss_epoch/len(train_loader):.4f}, D Loss: {d_loss_epoch/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Save the best model\n",
    "    torch.save(best_params, 'best_autoencoder_params.pth')\n",
    "    print(f'Best model saved with val_loss: {best_val_loss:.4f} at epoch {best_params[\"epoch\"]}')\n",
    "\n",
    "# Testing function\n",
    "def test_autoencoder(encoder, decoder, test_loader, data_depth=3):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in test_loader:\n",
    "            batch_size = images.size(0)\n",
    "            random_data = torch.rand(batch_size, data_depth, images.size(2), images.size(3), device=images.device)\n",
    "            encoded_images = encoder(images, random_data)\n",
    "            decoded_data = decoder(encoded_images)\n",
    "\n",
    "            # Here you can do further evaluation or save the images\n",
    "            # For simplicity, I'll print the first decoded data\n",
    "            print(decoded_data.shape)  # This is to verify the shape of the decoded data\n",
    "            break\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Load data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR10(root='data/', download=True, transform=transform)\n",
    "\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.15 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    encoder = Encoder(data_depth=3)\n",
    "    decoder = Decoder(data_depth=3)\n",
    "    loss_fn = AutoencoderLoss()\n",
    "\n",
    "    # Train the Autoencoder\n",
    "    train_autoencoder(encoder, decoder, loss_fn, train_loader, val_loader, num_epochs=20)\n",
    "\n",
    "    # Load the best model for testing\n",
    "    best_params = torch.load('best_autoencoder_params.pth')\n",
    "    encoder.load_state_dict(best_params['encoder_state_dict'])\n",
    "    decoder.load_state_dict(best_params['decoder_state_dict'])\n",
    "\n",
    "    # Test the Autoencoder (decode data)\n",
    "    test_autoencoder(encoder, decoder, test_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9e2047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/20], E Loss: 0.0817, D Loss: 0.0817, Val Loss: 0.0169, Time: 1093.36s\n",
      "Epoch [2/20], E Loss: 0.0125, D Loss: 0.0125, Val Loss: 0.0100, Time: 1170.90s\n",
      "Epoch [3/20], E Loss: 0.0090, D Loss: 0.0090, Val Loss: 0.0088, Time: 1336.99s\n",
      "Epoch [4/20], E Loss: 0.0075, D Loss: 0.0075, Val Loss: 0.0062, Time: 1030.88s\n",
      "Epoch [5/20], E Loss: 0.0066, D Loss: 0.0066, Val Loss: 0.0073, Time: 1085.47s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 236\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHiding and extraction completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 236\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[2], line 195\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    192\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m AutoencoderLoss()\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Train the Autoencoder\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m train_autoencoder(encoder, decoder, loss_fn, train_loader, val_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# Load the best model for testing\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_autoencoder_params.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[2], line 112\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[1;34m(encoder, decoder, loss_fn, train_loader, val_loader, num_epochs, lr, data_depth)\u001b[0m\n\u001b[0;32m    110\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    111\u001b[0m random_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(batch_size, data_depth, images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m), device\u001b[38;5;241m=\u001b[39mimages\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 112\u001b[0m encoded_images \u001b[38;5;241m=\u001b[39m encoder(images, random_data)\n\u001b[0;32m    113\u001b[0m decoded_data \u001b[38;5;241m=\u001b[39m decoder(encoded_images)\n\u001b[0;32m    114\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(decoded_data, random_data)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, image, data)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, data):\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([image, data], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Encoder model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, data_depth=1, hidden_size=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.data_depth = data_depth\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3 + data_depth, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_size, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, image, data):\n",
    "        x = torch.cat([image, data], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "# Decoder model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, data_depth=1, hidden_size=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.data_depth = data_depth\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_size, data_depth, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, encoded_image):\n",
    "        return self.model(encoded_image)\n",
    "\n",
    "# Custom loss function for the autoencoder\n",
    "class AutoencoderLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoencoderLoss, self).__init__()\n",
    "        self.pixelwise_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, decoded_data, real_data):\n",
    "        return self.pixelwise_loss(decoded_data, real_data)\n",
    "\n",
    "# Training function\n",
    "def train_autoencoder(encoder, decoder, loss_fn, train_loader, val_loader, num_epochs=20, lr=0.0002, data_depth=1):\n",
    "    encoder = encoder\n",
    "    decoder = decoder\n",
    "    loss_fn = loss_fn\n",
    "\n",
    "    optimizer_e = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    optimizer_d = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        e_loss_epoch = 0.0\n",
    "        d_loss_epoch = 0.0\n",
    "\n",
    "        for i, (images, _) in enumerate(train_loader):\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            random_data = torch.rand(batch_size, data_depth, images.size(2), images.size(3), device=images.device)\n",
    "\n",
    "            # Train Encoder and Decoder\n",
    "            optimizer_e.zero_grad()\n",
    "            optimizer_d.zero_grad()\n",
    "\n",
    "            # Encode and decode images\n",
    "            encoded_images = encoder(images, random_data)\n",
    "            decoded_data = decoder(encoded_images)\n",
    "\n",
    "            # Autoencoder loss\n",
    "            loss = loss_fn(decoded_data, random_data)\n",
    "            loss.backward()\n",
    "            optimizer_e.step()\n",
    "            optimizer_d.step()\n",
    "\n",
    "            e_loss_epoch += loss.item()\n",
    "            d_loss_epoch += loss.item()\n",
    "\n",
    "        # Validate\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, _ in val_loader:\n",
    "                batch_size = images.size(0)\n",
    "                random_data = torch.rand(batch_size, data_depth, images.size(2), images.size(3), device=images.device)\n",
    "                encoded_images = encoder(images, random_data)\n",
    "                decoded_data = decoder(encoded_images)\n",
    "                loss = loss_fn(decoded_data, random_data)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = {'lr': lr, 'epoch': epoch, 'encoder_state_dict': encoder.state_dict(), 'decoder_state_dict': decoder.state_dict()}\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], E Loss: {e_loss_epoch/len(train_loader):.4f}, D Loss: {d_loss_epoch/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Time: {epoch_time:.2f}s')\n",
    "\n",
    "    # Save the best model\n",
    "    with open('best_autoencoder_params.pkl', 'wb') as f:\n",
    "        pickle.dump(best_params, f)\n",
    "    print(f'Best model saved with val_loss: {best_val_loss:.4f} at epoch {best_params[\"epoch\"]}')\n",
    "\n",
    "# Testing function\n",
    "def test_autoencoder(encoder, decoder, test_loader, data_depth=1):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    all_decoded = []\n",
    "    all_random = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in test_loader:\n",
    "            batch_size = images.size(0)\n",
    "            random_data = torch.rand(batch_size, data_depth, images.size(2), images.size(3), device=images.device)\n",
    "            encoded_images = encoder(images, random_data)\n",
    "            decoded_data = decoder(encoded_images)\n",
    "\n",
    "            all_decoded.append(decoded_data.cpu().numpy())\n",
    "            all_random.append(random_data.cpu().numpy())\n",
    "\n",
    "    all_decoded = np.concatenate(all_decoded, axis=0)\n",
    "    all_random = np.concatenate(all_random, axis=0)\n",
    "\n",
    "    # Calculate F1 score, accuracy, and confusion matrix\n",
    "    f1 = f1_score(all_random.flatten(), all_decoded.flatten() > 0, average='weighted')\n",
    "    accuracy = accuracy_score(all_random.flatten(), all_decoded.flatten() > 0)\n",
    "    conf_matrix = confusion_matrix(all_random.flatten(), all_decoded.flatten() > 0)\n",
    "\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "    return f1, accuracy, conf_matrix\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Load data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR10(root='data/', download=True, transform=transform)\n",
    "\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.15 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    encoder = Encoder(data_depth=1)\n",
    "    decoder = Decoder(data_depth=1)\n",
    "    loss_fn = AutoencoderLoss()\n",
    "\n",
    "    # Train the Autoencoder\n",
    "    train_autoencoder(encoder, decoder, loss_fn, train_loader, val_loader, num_epochs=20)\n",
    "\n",
    "    # Load the best model for testing\n",
    "    with open('best_autoencoder_params.pkl', 'rb') as f:\n",
    "        best_params = pickle.load(f)\n",
    "    encoder.load_state_dict(best_params['encoder_state_dict'])\n",
    "    decoder.load_state_dict(best_params['decoder_state_dict'])\n",
    "\n",
    "    # Test the Autoencoder (decode data)\n",
    "    f1, accuracy, conf_matrix = test_autoencoder(encoder, decoder, test_loader)\n",
    "\n",
    "    # Example of hiding a greyscale image in a cover image\n",
    "    secret_image_path = 'C:/Users/Hendy Group/OneDrive/Desktop/final_graduation_project/12.jpg'\n",
    "    cover_image_path = 'C:/Users/Hendy Group/OneDrive/Desktop/final_graduation_project/11.jpg'\n",
    "\n",
    "\n",
    "    secret_image = transforms.ToTensor()(Image.open(secret_image_path).convert('L')).unsqueeze(0)\n",
    "    cover_image = transforms.ToTensor()(Image.open(cover_image_path).convert('RGB')).unsqueeze(0)\n",
    "\n",
    "    # Ensure the images are the same size\n",
    "    secret_image = transforms.Resize((64, 64))(secret_image)\n",
    "    cover_image = transforms.Resize((64, 64))(cover_image)\n",
    "\n",
    "    # Normalize the secret image\n",
    "    secret_image = (secret_image - 0.5) / 0.5\n",
    "\n",
    "    # Encode and decode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded_image = encoder(cover_image, secret_image)\n",
    "        decoded_image = decoder(encoded_image)\n",
    "\n",
    "    # Save the results\n",
    "    save_image(encoded_image, 'encoded_image.jpg')\n",
    "    save_image(decoded_image, 'decoded_image.jpg')\n",
    "\n",
    "    print('Hiding and extraction completed.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd8162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
